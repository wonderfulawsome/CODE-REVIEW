{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas \n",
    "!pip install scikit-learn \n",
    "!pip install numba \n",
    "!pip install tqdm \n",
    "!pip install matplotlib \n",
    "!pip install transformers\n",
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import random\n",
    "import os, re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import logging\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoModelForSequenceClassification\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from itertools import combinations\n",
    "from rank_bm25 import BM25L\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 자동으로 불러오기 위한 숫자 생성(001~500)\n",
    "numbers = [str(i).zfill(3) for i in range(1, 501)]\n",
    "all_code_list = []\n",
    "\n",
    "# train_code(cpp) 불러오기\n",
    "for i in numbers:\n",
    "    cpp_code_list = []\n",
    "    \n",
    "    for file in os.listdir(f\"./data/train_code/problem{i}\"):\n",
    "    \n",
    "        if file.endswith(\".cpp\"):\n",
    "            with open(os.path.join(f\"./data/train_code/problem{i}\", file), \"r\") as f:\n",
    "                cpp_code_list.append(f.read())\n",
    "    all_code_list.append(cpp_code_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ->지정된 디렉토리 내의 모든 C++ 코드 파일의 내용이 all_code_list에 순서대로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 1_주석 처리 \n",
    "def clean_data(text):\n",
    "\n",
    "    #입력된 문자열 text의 앞과 뒤에서 공백을 제거\n",
    "    text = text.strip()\n",
    "\n",
    "    #정규 표현식을 사용하여 C++의 한 줄 주석(//로 시작하는 부분)을 찾아서 제거\n",
    "    text = re.sub(r\"//.*\", \"\", text)\n",
    "\n",
    "    #C++의 여러 줄 주석(/ *로 시작하고 * /로 끝나는 부분, 여기서 공백은 없어야 함)을 찾아서 제거\n",
    "    text = re.sub(r'/\\*.*?\\*/', '', text, flags=re.DOTALL)\n",
    "\n",
    "    #다시 한번 문자열의 앞뒤 공백을 제거\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# 전처리 함수 2_빈 줄 제거\n",
    "def get_rid_of_empty(c):\n",
    "    ret = []\n",
    "    splitted = c.split('\\n')\n",
    "    for s in splitted:\n",
    "        if len(s.strip()) > 0:\n",
    "            ret.append(s)\n",
    "    return '\\n'.join(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 함수 1: clean_data\n",
    "이 함수는 주어진 텍스트에서 특정 패턴의 문자열을 제거하는 역할을 한다.\n",
    "\n",
    "전처리 함수 2: get_rid_of_empty\n",
    "이 함수는 주어진 문자열에서 빈 줄을 제거하는 역할을 한다.\n",
    "\n",
    "### ->C++ 코드에서 주석을 제거하고 빈 줄을 없애는 전처리 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 실행\n",
    "all_code_list_clean = []\n",
    "\n",
    "for i in range(500):\n",
    "    cleans = []\n",
    "    for j in range(500):\n",
    "        clean = get_rid_of_empty(clean_data(all_code_list[i][j]))\n",
    "        cleans.append(clean)\n",
    "    all_code_list_clean.append(cleans)\n",
    "\n",
    "-------------------------------------------------\n",
    "all_code_list_clean=[]\n",
    "\n",
    "for i in range(500):\n",
    "    cleans=[]\n",
    "    for j in range(500):\n",
    "        clean=get_rid_of_empty(clean_data(all_code_list[i][j]))\n",
    "        cleans.append(clean)\n",
    "    all_code_list_clean.append(cleans)\n",
    "\n",
    "all_code_list_clean=[]\n",
    "\n",
    "for i in range(500):\n",
    "    cleans=[]\n",
    "    for j in range(500):\n",
    "        clean=get_rid_of_empty(clean_data(all_code_list[i][j]))\n",
    "        cleans.append(clean)\n",
    "    all_code_list_clean.append(cleans)\n",
    "\n",
    "for i in range(500):\n",
    "    cleans=[]\n",
    "    for j in range(500):\n",
    "        clean=get_rid_of_empty(clean_data(all_code_list[i][j]))\n",
    "        cleans.append(clean)\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 개의 중첩 반복문 사용: 외부 반복문은 i를 0부터 499까지 반복하고, 내부 반복문은 j를 0부터 499까지 반복한다. 이는 총 250,000(500x500)회의 반복을 의미하며, all_code_list라는 2차원 리스트(또는 다른 형태의 중첩된 데이터 구조)의 각 원소에 접근하여 처리한다.\n",
    "\n",
    "전처리 함수 호출: clean_data() 함수와 get_rid_of_empty() 함수를 순차적으로 호출한다. 이 함수들은 각각의 이름에서 유추할 수 있듯이, 데이터를 정제하고 빈 값을 처리하는 역할을 한다. 정확한 작업 내용은 함수의 구현에 따라 다르지만, 일반적으로 불필요한 공백 제거, 잘못된 데이터 형식 수정, 결측치 처리 등의 작업을 포함할 수 있다.\n",
    "\n",
    "결과 저장: 각 j번째 반복에서 처리된 clean 데이터는 cleans라는 리스트에 추가된다. 이렇게 내부 반복문에서 생성된 cleans 리스트는 각 i번째 반복에서 all_code_list_clean에 추가된다. 최종적으로, all_code_list_clean는 원본 all_code_list의 각 원소를 전처리한 결과를 담은 2차원 리스트가 된다.\n",
    "\n",
    "### -> 모든 C++ 코드의 주석을 제거하고 빈 줄을 없애는 전처리를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 생성 config 및 함수\n",
    "\n",
    "# config\n",
    "#cfg 클래스를 정의하고, 생성자(__init__)에서 checkpoint_path 변수를 초기화합니다. 이 변수는 모델 체크포인트의 경로를 저장합니다.\n",
    "class cfg():\n",
    "    def __init__(self) :\n",
    "        self.checkpoint_path = 'neulab/codebert-cpp'\n",
    "        # self.learning_rate = 3e-4\n",
    "        # self.epochs = 5\n",
    "        # self.num_labels=2\n",
    "        # self.batch_size=16\n",
    "class cfg():\n",
    "    def __init__(self):\n",
    "        self.checkpoint_path='neulab/codebert-cpp'\n",
    "args = cfg()\n",
    "\n",
    "# 함수\n",
    "#입력 데이터프레임(input_df)에서 'code' 열의 모든 값을 리스트로 추출합니다.\n",
    "#고유한 'problem_num' 값을 찾아 리스트로 만들고, 이 리스트를 정렬합니다.\n",
    "\n",
    "#######code를 리스트로 -> 리스트를 정렬\n",
    "def get_pairs(input_df, tokenizer):\n",
    "    codes = input_df['code'].to_list()\n",
    "    problems = input_df['problem_num'].unique().tolist()\n",
    "    problems.sort()\n",
    "\n",
    "def get_pairs(input_df,tokenizer):\n",
    "    codes=Winput_df['code'].to_list()\n",
    "    problems=input_df['problem_num'].unique().tolist()\n",
    "    problems.sort()\n",
    "    \n",
    "#입력 코드를 토큰화하고, 이 토큰화된 코드들을 사용하여 BM25L 객체를 초기화합니다. BM25L은 각 코드와 다른 코드 간의 유사성 점수를 계산하는 데 사용됩니다.\n",
    "    \n",
    "    ######## BM25L 생성\n",
    "    tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "    bm25 = BM25L(tokenized_corpus)\n",
    "\n",
    "    tokenized_corpous=[tokenizer.tokenize(code) for code in codes]\n",
    "    bm25=BM25L(tokenized_corpus)\n",
    "\n",
    "#유사한 쌍과 유사하지 않은 쌍을 저장할 리스트를 초기화\n",
    "    total_positive_pairs = []\n",
    "    total_negative_pairs = []\n",
    "\n",
    "#현재 문제 번호(problem)에 해당하는 모든 코드를 선택합니다. 선택된 코드들 사이에서 가능한 모든 유사한 쌍(positive_pairs)을 생성합니다\n",
    "    \n",
    "    ##### 유사한 쌍 생성\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes = input_df[input_df['problem_num'] == problem]['code']\n",
    "        positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes= input_df[input_df['problem_num']==problem]['code']\n",
    "        positive_pairs=list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes=input_df[input_df['problem_num']==problem]['code']\n",
    "        positive_codes=list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "#선택된 코드의 인덱스를 저장하고, 유사하지 않은 쌍을 저장할 빈 리스트를 초기화합니다.\n",
    "        solution_codes_indices = solution_codes.index.to_list()\n",
    "        negative_pairs = []\n",
    "\n",
    "#첫 번째 유사한 쌍의 첫 번째 코드를 토큰화하고, BM25 점수를 계산한 후, 이 점수를 기준으로 내림차순 정렬하여 가장 유사도가 높은 코드의 인덱스를 구합니다.\n",
    "        first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "        negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "        negative_code_ranking = negative_code_scores.argsort()[::-1] # 내림차순\n",
    "        #유사도 순위에 사용할 인덱스 변수를 초기화합니다.\n",
    "        ranking_idx = 0\n",
    "\n",
    "        first_tokenized_code=tokenizer.tokenize(positive_pairs[0][0])\n",
    "        negative_code_scores=bm25.get_scores(first_tokenized_code) # bm25점수 계산\n",
    "        negative_code_ranking=negative_code_scores.argsort()[::-1]#내림차순\n",
    "\n",
    "#현재 문제의 모든 해결책 코드에 대해 반복\n",
    "        for solution_code in solution_codes:\n",
    "            negative_solutions = []\n",
    "            while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "                high_score_idx = negative_code_ranking[ranking_idx]\n",
    "\n",
    "                if high_score_idx not in solution_codes_indices:\n",
    "                    negative_solutions.append(input_df['code'].iloc[high_score_idx])\n",
    "                ranking_idx += 1\n",
    "\n",
    "            for negative_solution in negative_solutions:\n",
    "                negative_pairs.append((solution_code, negative_solution))\n",
    "\n",
    "        total_positive_pairs.extend(positive_pairs)\n",
    "        total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "    pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "    pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "    neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "    neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "    pos_label = [1]*len(pos_code1)\n",
    "    neg_label = [0]*len(neg_code1)\n",
    "\n",
    "    pos_code1.extend(neg_code1)\n",
    "    total_code1 = pos_code1\n",
    "    pos_code2.extend(neg_code2)\n",
    "    total_code2 = pos_code2\n",
    "    pos_label.extend(neg_label)\n",
    "    total_label = pos_label\n",
    "    pair_data = pd.DataFrame(data={\n",
    "        'code1':total_code1,\n",
    "        'code2':total_code2,\n",
    "        'similar':total_label\n",
    "    })\n",
    "    pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "    return pair_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ->입력 코드 샘플들 사이에서 유사한 쌍과 유사하지 않은 쌍을 찾아내어, 코드 유사도를 학습할 수 있는 데이터셋을 생성\n",
    "\n",
    "1. 설정(config)\n",
    "cfg 클래스를 통해 모델 학습과 데이터 처리에 필요한 설정값을 정의한다. 여기에는 checkpoint_path 등의 모델 설정이 포함되며, 주석 처리된 부분에는 학습률(learning_rate), 에폭(epochs), 레이블 수(num_labels), 배치 크기(batch_size) 등의 학습 관련 설정이 나타난다.\n",
    "\n",
    "2. 데이터 셋 생성 함수(get_pairs)\n",
    "get_pairs 함수는 입력 데이터프레임(input_df)과 토크나이저(tokenizer)를 받아, 코드 쌍과 해당 쌍이 유사한지(양성) 혹은 다른지(음성)를 나타내는 레이블을 가진 데이터셋을 생성한다.\n",
    "\n",
    "코드 및 문제 처리: 입력 데이터에서 코드(code)와 문제 번호(problem_num)를 추출하여 리스트로 만든다. 문제 번호는 고유한 값을 추출하여 정렬한다.\n",
    "\n",
    "토큰화 및 BM25 적용: 입력 코드를 토크나이저를 사용해 토큰화하고, 토큰화된 코드를 기반으로 BM25 알고리즘을 사용해 유사도 점수를 계산한다. BM25는 검색 엔진 등에서 사용되는 랭킹 함수로, 문서의 중요한 특성을 고려하여 유사도를 평가한다.\n",
    "\n",
    "양성 및 음성 쌍 생성:\n",
    "\n",
    "양성 쌍: 같은 문제에 대한 서로 다른 솔루션 코드를 쌍으로 묶어 양성 쌍을 생성한다.\n",
    "음성 쌍: 양성 쌍을 구성하는 코드 중 하나와 다른 문제의 솔루션 코드를 쌍으로 묶어 음성 쌍을 생성한다. BM25 점수를 기반으로 유사도가 높은 코드를 선택하되, 동일한 문제의 솔루션은 제외한다.\n",
    "데이터프레임 생성 및 셔플링: 양성 및 음성 쌍으로부터 추출한 코드와 레이블을 합쳐 최종 데이터프레임을 생성한다. 데이터프레임은 전체를 무작위로 섞어(sample(frac=1)) 모델 학습에 사용될 수 있도록 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 셋 만들기 (각 코드 : 문제 번호)\n",
    "#초기화: 빈 리스트 preproc_scripts와 problem_nums를 생성합니다. 이 리스트들은 전처리된 스크립트와 각 스크립트에 해당하는 문제 번호를 저장하는 데 사용\n",
    "preproc_scripts = []\n",
    "problem_nums = []\n",
    "\n",
    "#i는 문제 번호의 인덱스, k는 특정 문제 번호에 해당\n",
    "for i in range(500):\n",
    "    for k in range(500):\n",
    "\n",
    "        for i in range(500):\n",
    "            for k in range(500):\n",
    "        ##all_code_list_clean[i][k]를 사용하여 전처리된 스크립트를 가져온 후, preproc_scripts 리스트에 추가합니다. 여기서 preprocessed_script는 i번째 문제의 k번째 코드를 전처리한 결과입니다.\n",
    "        preprocessed_script = all_code_list_clean[i][k]\n",
    "        preproc_scripts.append(preprocessed_script)\n",
    "\n",
    "        preprocess_script=all_code_list_clean[i][k]\n",
    "        preproc_scripts.append(preprocess_script)\n",
    "        #문제 번호 저장\n",
    "        problem_nums.append(numbers[i])\n",
    "\n",
    "#preproc_scripts와 problem_nums 리스트를 사용하여 새로운 pandas 데이터프레임 data_df를 생성\n",
    "data_df = pd.DataFrame(data={'code': preproc_scripts, 'problem_num': problem_nums})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 전처리된 스크립트와 해당 문제 번호를 포함하는 학습 데이터셋을 만드는 과정전처리된 스크립트와 해당 문제 번호를 포함하는 학습 데이터셋을 만드는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 생성 config 및 함수\n",
    "\n",
    "# config\n",
    "#cfg 클래스를 정의하고, 생성자(__init__)에서 checkpoint_path 변수를 초기화합니다. 이 변수는 모델 체크포인트의 경로를 저장합니다.\n",
    "class cfg():\n",
    "    def __init__(self) :\n",
    "        self.checkpoint_path = 'neulab/codebert-cpp'\n",
    "        # self.learning_rate = 3e-4\n",
    "        # self.epochs = 5\n",
    "        # self.num_labels=2\n",
    "        # self.batch_size=16\n",
    "\n",
    "args = cfg()\n",
    "\n",
    "# 함수\n",
    "#입력 데이터프레임(input_df)에서 'code' 열의 모든 값을 리스트로 추출합니다.\n",
    "#고유한 'problem_num' 값을 찾아 리스트로 만들고, 이 리스트를 정렬합니다.\n",
    "def get_pairs(input_df, tokenizer):\n",
    "    codes = input_df['code'].to_list()\n",
    "    problems = input_df['problem_num'].unique().tolist()\n",
    "    problems.sort()\n",
    "\n",
    "def get_pairs(input_df,tokenizer):\n",
    "    codes=input_df['code'].to_list()\n",
    "    problems=input_df['problem_num'].unique().tolist()\n",
    "    \n",
    "#입력 코드를 토큰화하고, 이 토큰화된 코드들을 사용하여 BM25L 객체를 초기화합니다. BM25L은 각 코드와 다른 코드 간의 유사성 점수를 계산하는 데 사용됩니다.\n",
    "    tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "    bm25 = BM25L(tokenized_corpus)\n",
    "\n",
    "    tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "    bm25=BM25L(tokenized_corpus)\n",
    "\n",
    "#유사한 쌍과 유사하지 않은 쌍을 저장할 리스트를 초기화\n",
    "    total_positive_pairs = []\n",
    "    total_negative_pairs = []\n",
    "\n",
    "    total_positive_pairs=[]\n",
    "    total_negative_pairs=[]\n",
    "\n",
    "#현재 문제 번호(problem)에 해당하는 모든 코드를 선택합니다. 선택된 코드들 사이에서 가능한 모든 유사한 쌍(positive_pairs)을 생성합니다\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes = input_df[input_df['problem_num'] == problem]['code']\n",
    "        positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes=input_df[input_df['problem_num']==problem]['code']\n",
    "        positive_paris=list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "#선택된 코드의 인덱스를 저장하고, 유사하지 않은 쌍을 저장할 빈 리스트를 초기화합니다.\n",
    "        solution_codes_indices = solution_codes.index.to_list()\n",
    "        negative_pairs = []\n",
    "\n",
    "        solution_codes_indices=solution_codes.index.to_list()\n",
    "        negative_pairs=[]\n",
    "\n",
    "#첫 번째 유사한 쌍의 첫 번째 코드를 토큰화하고, BM25 점수를 계산한 후, 이 점수를 기준으로 내림차순 정렬하여 가장 유사도가 높은 코드의 인덱스를 구합니다.\n",
    "        first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "        negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "        negative_code_ranking = negative_code_scores.argsort()[::-1] # 내림차순\n",
    "\n",
    "        first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "        negative_code_scores=bm25.get_scores(first_tokenized_code)\n",
    "        negative_code_ranking=negative_code_scores.argsort()[::-1]\n",
    "        #유사도 순위에 사용할 인덱스 변수를 초기화합니다.\n",
    "        ranking_idx = 0\n",
    "\n",
    "#현재 문제의 모든 해결책 코드에 대해 반복\n",
    "        for solution_code in solution_codes:\n",
    "            negative_solutions = []\n",
    "            while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "                high_score_idx = negative_code_ranking[ranking_idx]\n",
    "\n",
    "                if high_score_idx not in solution_codes_indices:\n",
    "                    negative_solutions.append(input_df['code'].iloc[high_score_idx])\n",
    "                ranking_idx += 1\n",
    "\n",
    "            for negative_solution in negative_solutions:\n",
    "                negative_pairs.append((solution_code, negative_solution))\n",
    "\n",
    "        total_positive_pairs.extend(positive_pairs)\n",
    "        total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "    pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "    pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "    neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "    neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "    pos_label = [1]*len(pos_code1)\n",
    "    neg_label = [0]*len(neg_code1)\n",
    "\n",
    "    pos_code1.extend(neg_code1)\n",
    "    total_code1 = pos_code1\n",
    "    pos_code2.extend(neg_code2)\n",
    "    total_code2 = pos_code2\n",
    "    pos_label.extend(neg_label)\n",
    "    total_label = pos_label\n",
    "    pair_data = pd.DataFrame(data={\n",
    "        'code1':total_code1,\n",
    "        'code2':total_code2,\n",
    "        'similar':total_label\n",
    "    })\n",
    "    pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "    return pair_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 입력 데이터프레임에서 코드 쌍을 생성하여 이들이 서로 유사한지 여부를 판단하는 데 필요한 데이터셋을 구성하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 분리\n",
    "train_code, valid_code, train_label, valid_label = train_test_split(\n",
    "    data_df,\n",
    "    data_df['problem_num'],\n",
    "    random_state=42,\n",
    "    test_size=0.1,\n",
    "    stratify=data_df['problem_num']\n",
    ")\n",
    "\n",
    "train_code, valid_code, train_label, valid_label = train_test_split(\n",
    "    data_df,\n",
    "    data_df['problem_num'],\n",
    "    random_state=42,\n",
    "    test_size=0.1,\n",
    "    stratify=data_df['problem_num']\n",
    ")\n",
    "\n",
    "#reset_index 메소드를 사용하여 학습 세트와 검증 세트의 인덱스를 재설정\n",
    "train_code = train_code.reset_index(drop=True)\n",
    "valid_code = valid_code.reset_index(drop=True)\n",
    "\n",
    "train_code=train_code.reset_index(drop=True)\n",
    "valide_code=valid_code.reset_index(drop=True)\n",
    "#from_pretrained 메소드를 사용하여 사전에 학습된 토크나이저를 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "#(자르기) 방향을 설정\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "#train_code 데이터프레임 내의 코드 쌍을 생성\n",
    "final_train_df = get_pairs(train_code, tokenizer)\n",
    "# final_valid_df = get_pairs(valid_code, tokenizer)\n",
    "\n",
    "# 생성 데이터 저장\n",
    "final_train_df.to_pickle(\"./data/train.pkl\")\n",
    "# final_valid_df.to_pickle(\"./data/val.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ->사전 학습된 토크나이저를 사용하여 코드 쌍을 생성하고, 이를 훈련 및 검증 데이터셋으로 나누는 과정을 담당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 셋 만들기\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "#code1'과 'code2'라는 열의 값을 numpy 배열로 추출\n",
    "code1 = test_df['code1'].values\n",
    "code2 = test_df['code2'].values\n",
    "\n",
    "code1=test_df['code1'].values\n",
    "code2=test_df['code2'].values\n",
    "#처리된 코드를 저장할 두 개의 빈 리스트를 초기화\n",
    "processed_code1 = []\n",
    "processed_code2 = []\n",
    "\n",
    "#code1' 배열의 길이만큼 반복문을 실행하여, 각 코드 쌍에 대해 전처리를 수행\n",
    "for i in range(len(code1)):\n",
    "for i in range(len(code1)):\n",
    "\n",
    "        #code1[i]'와 'code2[i]'에 대해 clean_data 함수를 호출하여 데이터를 청소한 후, get_rid_of_empty 함수를 호출하여 비어 있는 부분을 제거\n",
    "        processed_c1 = get_rid_of_empty(clean_data(code1[i]))\n",
    "        processed_c2 = get_rid_of_empty(clean_data(code2[i]))\n",
    "        \n",
    "        processed_c1=get_rid_of_empty(clean_data(code1[i]))\n",
    "        processed_c2=get_rid_of_empty(clean_data(code2[i]))\n",
    "\n",
    "        #processed_code1과 processed_code2 리스트에 추가\n",
    "        processed_code1.append(processed_c1)\n",
    "        processed_code2.append(processed_c2)\n",
    "        \n",
    "#새 DataFrame을 만듭니다.\n",
    "processed_test = pd.DataFrame(list(zip(processed_code1, processed_code2)), columns=[\"code1\", \"code2\"])\n",
    "processed_test.to_pickle(\"./data/processed_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> 테스트 데이터의 빈 공간을 제거하여 전처리 하고 새로운 dataframe 을 만드는 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "train_data = pd.read_pickle(\"./data/train.pkl\")\n",
    "# val_data = pd.read_pickle(\"./data/val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 추출(랜덤)\n",
    "#data_splitter라는 함수를 정의합니다. 이 함수는 데이터프레임 df와 추출할 샘플 크기 size를 입력\n",
    "def data_splitter(df, size):\n",
    "\n",
    "def data_splitter(df,size):\n",
    "\n",
    "    # 'similar' 열의 값이 0인 샘플들 중에서 무작위로 size // 2 만큼 추출\n",
    "    label_0_df = df[df['similar'] == 0].sample(size // 2)\n",
    "    label_0_idx = label_0_df.index\n",
    "\n",
    "    label_0_df=df[df['similar']==0].sample(size//2)\n",
    "    label_0_idx=label_0_df.index\n",
    "\n",
    "    label_0_df=df[df['similar']==0].sample(size//2)\n",
    "    label_0_idx=label_0_df.index\n",
    "\n",
    "    #'similar' 값이 1인 샘플들을 무작위로 size // 2 만큼 추출\n",
    "    label_1_df = df[df['similar'] == 1].sample(size // 2)\n",
    "    label_1_idx = label_1_df.index\n",
    "\n",
    "    #label_0_df와 label_1_df 데이터프레임을 하나로 합쳐 sampled_df라는 새로운 데이터프레임을 생성\n",
    "    sampled_df = pd.concat([label_0_df, label_1_df], axis=0)\n",
    "\n",
    "    sampled_df=pd.concat([label_0_df,label_1_df],axis=0)\n",
    "    smpled_df=pd.concat([label_0_df,label_1_df],axis=0)\n",
    "    \n",
    "    #label_0_idx와 label_1_idx 리스트를 합쳐서 sampled_idx라는 새로운 리스트를 생성\n",
    "    sampled_idx = list(label_0_idx) + list(label_1_idx)\n",
    "\n",
    "    return sampled_idx, sampled_df\n",
    "\n",
    "# 추출한 후 데이터 셋을 초기화\n",
    "def splitted_original(origin, idx):\n",
    "    origin = origin.drop(idx)\n",
    "    return origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 이 두 함수는 데이터셋을 레이블별로 균등하게 분할하고, 특정 부분을 추출한 후 원본 데이터셋에서 해당 부분을 제거하는 전처리 작업을 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 5000000\n",
    "\n",
    "for i in range(1, 9):\n",
    "\n",
    "    #train_size 만큼의 샘플을 랜덤하게 추출\n",
    "    train_idx, train_df = data_splitter(train_data, train_size)\n",
    "    \n",
    "    train_idx,train_df=data_splitter(train_data,train_size)\n",
    "\n",
    "    train_idx,traind_df=data_splitter(train_data,train_size)\n",
    "\n",
    "    #train_idx에 해당하는 샘플을 제거\n",
    "    train_data = splitted_original(train_data, train_idx)\n",
    "\n",
    "    train_data=splitted_original(train_data,train_idx)\n",
    "    train_df.to_pickle(f'./data/train{i}.pkl')\n",
    "\n",
    "    train_data=splitted_orginal(tarin_data,train_idx)\n",
    "    train_df.to_prickle(f'./data/train{i}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 이 프로세스를 통해 큰 데이터셋을 여러 개의 작은 파일로 분할하여 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_size = 100000\n",
    "\n",
    "# valid_idx, valid_df = data_splitter(val_data, val_size)\n",
    "# val_data = splitted_original(val_data, valid_idx)\n",
    "# valid_df.to_pickle(f'./data/valid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "- 모델은 neulab/codebert-cp\n",
    "- 다른 데이터 8개로 모델 파인 튜닝\n",
    "- DataParallel 시 loss 부분이랑 모델 저장 부분에서 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.source_len=512\n",
    "        self.epochs = 1\n",
    "\n",
    "        #학습률(learning rate)을 0.00002로 설정\n",
    "        self.learning_rate=2e-5\n",
    "\n",
    "        #배치 크기(batch size)를 32로 설정\n",
    "        self.batch_size=32\n",
    "\n",
    "        #데이터를 섞을지 여부를 결정하는 shuffle 플래그를 True로 설정\n",
    "        self.shuffle = True\n",
    "\n",
    "        #랜덤 시드를 2022로 설정\n",
    "        self.seed=2022\n",
    "\n",
    "        #모델의 출력 레이블 수를 2로 설정\n",
    "        self.num_labels=2\n",
    "        self.checkpoint_path = 'neulab/codebert-cpp'\n",
    "        self.train_path1 = './data/train1.pkl'\n",
    "        # self.train_path2 = './data/train2.pkl'\n",
    "        # self.train_path3 = './data/train3.pkl'\n",
    "        # self.train_path4 = './data/train4.pkl'\n",
    "        # self.train_path5 = './data/train5.pkl'\n",
    "        # self.train_path6 = './data/train6.pkl'\n",
    "        # self.train_path7 = './data/train7.pkl'\n",
    "        # self.train_path8 = './data/train8.pkl'\n",
    "        \n",
    "        # self.hf_data_path1= 'emaeon/train1'\n",
    "        # self.hf_data_path2= 'emaeon/train2'\n",
    "        # self.hf_data_path3= 'emaeon/train3'\n",
    "        # self.hf_data_path4= 'emaeon/train4'\n",
    "        # self.hf_data_path5= 'emaeon/train5'\n",
    "        # self.hf_data_path6= 'emaeon/train6'\n",
    "        # self.hf_data_path7= 'emaeon/train7'\n",
    "        # self.hf_data_path8= 'emaeon/train8'\n",
    "\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 이 클래스 인스턴스는 머신 러닝 모델을 학습시킬 때 필요한 다양한 구성 옵션을 저장하는데 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.bechmark = True\n",
    "\n",
    "seed_everything(cfg.seed) #seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델, 토크나이저 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스에서 사전학습된 모델 불러옵니다\n",
    "\n",
    "#이 모델은 시퀀스 분류 작업(예: 문장이나 문서의 분류)에 사용됩니다\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.checkpoint_path, num_labels=cfg.num_labels, output_hidden_states=False).to(device)\n",
    "\n",
    "model= AutoModelForSequenceClassification.from_pretrained(cfg.checkpoint_path,num_labels=cfg.num_labels,output_hidden_states=False).to(device)\n",
    "\n",
    "#토크나이저는 텍스트를 모델이 처리할 수 있는 형태로 변환하는 데 사용\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.checkpoint_path,)\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(cfg.checkpoint_path)\n",
    "\n",
    "#토크나이저의 트렁케이션(자르기) 방향을 \"left\"로 설정\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "tokenizer.truncation_side='left'\n",
    "\n",
    "tokenizer.truncation_side='left'\n",
    "\n",
    "#모델의 토큰 임베딩 크기를 토크나이저의 어휘 크기에 맞게 조정\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "'''DataParallel이 필요할 경우에는 아래 코드를 실행해야 합니다'''\n",
    "# model = nn.DataParallel(model).to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ->  사전 학습된 모델과 토크나이저를 로드하고, 모델을 현재 작업 환경에 맞게 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model #모델 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 커스터마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    #클래스의 생성자(__init__)는 데이터셋을 초기화할 때 필요한 파라미터들을 받습니다.\n",
    "    def __init__(self, data_a,data_b, labels, tokenizer, source_len) :\n",
    "\n",
    "    def __init__(self,data_a,data b, labels, tokenizer, source_len):\n",
    "\n",
    "    # 초기화 과정에서 입력으로 받은 값들을 클래스 인스턴스의 속성으로 복사하고 저장\n",
    "        self.data_a = data_a.copy()\n",
    "        self.data_b = data_b.copy()\n",
    "        self.labels = labels.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_len = source_len\n",
    "\n",
    "        self.data_a= data_a.copy()\n",
    "        self.data_b=data_b.copy()\n",
    "        self.labels=labels.copy()\n",
    "        self.tokenizer=tokenizer\n",
    "        self.source_len=source_len\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "    # 데이터 셋에서 한 개의 데이터를 가져오는 함수 정의\n",
    "        text1 = self.data_a[index]\n",
    "        text2 = self.data_b[index]\n",
    "\n",
    "        text1=self.data_a[index]\n",
    "        text2= self.data_b[index]\n",
    "        \n",
    "        '''text_pair에 비교할 문장을 입력하면 알아서 sep토큰으로 문장 구분된 하나의 입력 셋이 형성됩니다.'''\n",
    "        #tokenizer를 사용하여 두 텍스트(text1, text2)를 토크나이징\n",
    "        inputs = self.tokenizer(text = text1,text_pair=text2,max_length=self.source_len,padding='max_length',truncation=True, return_tensors='pt') \n",
    "        label = self.labels[index]\n",
    "\n",
    "        inputs= self.tokenizer(text=text, text_pair=text2, max_length=self.source_len,pedding='max_length',truncation=True,return_tensors='pt')\n",
    "        label=self.labels[index]\n",
    "\n",
    "        '''neulab/codebert-cpp는 input_ids와 attention_mask를 입력 받습니다'''\n",
    "        #input_ids와 attention_mask를 추출하고, .squeeze() 메소드를 통해 불필요한 차원을 제거\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        input_ids=inputs['input_ids'].squueze()\n",
    "        attention_mask=inputs['input_ids'].squeeze()\n",
    "\n",
    "        #input_ids와 attention_mask를 딕셔너리로 구성하고, .to(device)를 통해 해당 텐서들을 계산을 수행할 장치(CPU 또는 GPU)로 이동\n",
    "        inputs_dict = {\n",
    "            'input_ids' : input_ids.to(device, dtype = torch.long),\n",
    "            'attention_mask' : attention_mask.to(device, dtype = torch.long),\n",
    "        }\n",
    "        label = torch.tensor(label).to(device, dtype = torch.long),\n",
    "    \n",
    "        return inputs_dict, label #\n",
    "\n",
    "        inputs_dict = {\n",
    "            'input_ids' : input_ids.to(device, dtype = torch.long),\n",
    "            'attention_mask' : attention_mask.to(device, dtype = torch.long),\n",
    "        }\n",
    "        label = torch.tensor(label).to(device, dtype = torch.long),\n",
    "    \n",
    "        return inputs_dict, label #\n",
    "\n",
    "    def __len__(self) :\n",
    "    # 데이터 셋의 길이\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  -> 두텍스트 사이의 관계나 유사도를 파악하는 클래스를 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_pickle(cfg.train_path1)\n",
    "# traindf = pd.read_pickle(cfg.train_path2)\n",
    "# traindf = pd.read_pickle(cfg.train_path3)\n",
    "# traindf = pd.read_pickle(cfg.train_path4)\n",
    "# traindf = pd.read_pickle(cfg.train_path5)\n",
    "# traindf = pd.read_pickle(cfg.train_path6)\n",
    "# traindf = pd.read_pickle(cfg.train_path7)\n",
    "# traindf = pd.read_pickle(cfg.train_path8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"허깅페이스에 올려놓은 데이터를 가져오려면 다음과 같이 해야 합니다\"\"\"\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(cfg.hf_data_path1)\n",
    "# traindf = dataset['train'].to_pandas()\n",
    "\n",
    "\n",
    "# dataset = load_dataset(cfg.hf_data_path2)\n",
    "# dataset = load_dataset(cfg.hf_data_path3)\n",
    "# dataset = load_dataset(cfg.hf_data_path4)\n",
    "# dataset = load_dataset(cfg.hf_data_path5)\n",
    "# dataset = load_dataset(cfg.hf_data_path6)\n",
    "# dataset = load_dataset(cfg.hf_data_path7)\n",
    "# dataset = load_dataset(cfg.hf_data_path8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(data_a=list(traindf['code1']),data_b=list(traindf['code2']),\n",
    "                           labels=list(traindf['similar']),tokenizer=tokenizer,source_len=cfg.source_len)\n",
    "train_loader = DataLoader(train_data, batch_size=cfg.batch_size, shuffle=cfg.shuffle,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elapsed라는 인자를 받는데, 이는 경과 시간을 초 단위로 나타낸 값\n",
    "def format_time(elapsed):\n",
    "\n",
    "    #입력된 경과 시간(elapsed)을 가장 가까운 정수로 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    #datetime.timedelta 객체는 일, 시간, 분, 초를 나타내는데 사용\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed))\n",
    "    reuturn str(datetime.timedelta(seconds=elapsed_rounded)))\n",
    "    \n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded=int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 경과 시간(초 단위)을 받아서, 이를 HH:MM:SS 형태의 문자열로 포맷팅하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, loader):\n",
    "\n",
    "    #모델을 학습 모드로 설정\n",
    "    model.train()\n",
    "\n",
    "    #총 손실(total_loss), 총 정확도(total_accuracy), 그리고 학습 스텝 수(nb_train_steps)를 초기화\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    nb_train_steps = 0\n",
    "    \n",
    "    #tqdm은 진행 상황을 시각적으로 보여주는 라이브러리\n",
    "    for _,(inputs, labels) in tqdm(enumerate(loader, 0)):\n",
    "        \n",
    "        #모델에 입력 데이터(inputs)와 레이블(labels)을 전달하여 포워드 패스를 실행\n",
    "        outputs = model(**inputs, labels = labels)\n",
    "\n",
    "        #outputs 딕셔너리에서 손실 값을 추출\n",
    "        loss = outputs.loss\n",
    "        '''DataParallel은 loss가 3개가 나오기 때문에 평균해야 합니다'''\n",
    "        # loss = outputs.loss.mean()\n",
    "\n",
    "        #모델 출력(outputs.logits)에서 예측된 클래스를 추출하고, 실제 레이블과 비교하여 정확도를 계산\n",
    "        pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "        true = [label for label in labels.cpu().numpy()]\n",
    "        acc = accuracy_score(true,pred)\n",
    "\n",
    "        #매 50번째 스텝마다 현재의 평균 손실과 경과 시간을 출력\n",
    "        if _ % 50 == 0 and not _ == 0: #50iter마다 loss 확인하고자 넣었습니다.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(_, len(loader), elapsed))\n",
    "            print('  current average loss = {}'.format(\n",
    "                total_loss / _))\n",
    "        if _%50==0  and not _ == 0 :\n",
    "            print(f'Epoch : {epoch}, train_{_}_step_loss : {loss.item()}')\n",
    "            psuedo_pred = [logit.argmax().item() for logit in outputs.logits]\n",
    "            psuedo_acc = np.sum(np.array(labels.to('cpu'))==np.array(psuedo_pred))/len(labels)\n",
    "            print(f'{epoch}_{_}_step_정확도 :{psuedo_acc}')\n",
    "        if _%15625==0 and not _ == 0: #런타임 오류가 생길 경우 모델이 날라갈 것을 방지하고자 했습니다.\n",
    "            torch.save(model.state_dict(), f'/data/{_}batch_trained_cppbert1.pt')\n",
    "            '''DataParallel후 저장하는 방법이 약간 다릅니다'''\n",
    "            # torch.save(model.module.state_dict(), f'/data/{_}batch_trained_cppbert.pt')\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #총 손실, 총 정확도를 업데이트하고, 학습 스텝 수를 증가\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += acc\n",
    "        nb_train_steps += 1\n",
    "\n",
    "    #전체 손실(total_loss)을 데이터 로더(loader)의 길이(즉, 총 배치 수)로 나누어 평균 손실(avg_loss)을 계산\n",
    "    avg_loss = total_loss/len(loader)\n",
    "\n",
    "    #총 정확도(total_accuracy)를 학습 스텝 수(nb_train_steps)로 나누어 평균 정확도(avg_acc)를 계산\n",
    "    avg_acc = total_accuracy/nb_train_steps\n",
    "\n",
    "    #총 정확도(total_accuracy)를 데이터 로더(loader)의 길이로 나누어 다른 형태의 평균 정확도(t_test_avg_acc)를 계산\n",
    "    t_test_avg_acc = total_accuracy/len(loader)\n",
    "\n",
    "    print(f'Epoch:{epoch}, train_{_}_stepLoss:{avg_loss}')\n",
    "    print(f'Epoch:{epoch}, train_{_}_stepacc:{avg_acc}')\n",
    "    print(f'Epoch:{epoch}, train_{_}_stepacc:{t_test_avg_acc}')\n",
    "    loss_dic['train_loss'].append(avg_loss)\n",
    "    loss_dic['train_acc'].append(avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 모델 학습 과정에서 손실을 계산하고, 가중치를 업데이트하며, 진행 상황을 모니터링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW 최적화 알고리즘의 인스턴스를 생성\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "#코사인 애닐링 웜 리스타트(Cosine Annealing Warm Restarts) 학습률 스케줄러의 인스턴스를 생성\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.01, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 고급 최적화 기법과 학습률 조정 전략을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dic = {'epoch':[],'train_loss':[], 'validation_loss':[],'train_acc':[],'val_acc':[]}\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(1,cfg.epochs+1)):\n",
    "    t0 = time.time()\n",
    "    train(epoch, model, optimizer, train_loader)\n",
    "    torch.save(model.state_dict(), './data/trained_cppbert1.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert2.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert3.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert4.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert5.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert6.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert7.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert8.pt')\n",
    "    \n",
    "    '''DataParallel후 저장하는 방법이 약간 다릅니다'''\n",
    "    # torch.save(model.module.state_dict(), './data/trained_cppbert1.pt')  \n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 설정된 에폭 수만큼 모델을 학습시키고, 각 에폭마다 모델의 상태를 저장한 후, 학습률 스케줄러를 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')if torch.cuda.is_available() else torch.device('cpu')\n",
    "device\n",
    "\n",
    "#CUDA 장치(즉, GPU)를 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "\n",
    "        #입력 데이터의 최대 길이를 512로 설정\n",
    "        self.source_len=512\n",
    "\n",
    "        #학습 또는 추론을 위해 한 번에 처리할 데이터 샘플의 수를 16으로 설정\n",
    "        self.batch_size=16\n",
    "\n",
    "        #데이터를 모델에 공급하기 전에 데이터를 섞을지 여부를 결정\n",
    "        self.shuffle = True\n",
    "\n",
    "        #실험의 재현성을 위해 난수 생성 시 사용되는 시드 값을 2022로 설정\n",
    "        self.seed=2022\n",
    "\n",
    "        #모델이 예측해야 하는 레이블의 수를 2로 설정\n",
    "        self.num_labels=2\n",
    "        \n",
    "        self.load_path1= './data/trained_cppbert1.pt'\n",
    "        # self.load_path2= './data/trained_cppbert2.pt'\n",
    "        # self.load_path3= './data/trained_cppbert3.pt'\n",
    "        # self.load_path4= './data/trained_cppbert4.pt'\n",
    "        # self.load_path5= './data/trained_cppbert5.pt'\n",
    "        # self.load_path6= './data/trained_cppbert6.pt'\n",
    "        # self.load_path7= './data/trained_cppbert7.pt'\n",
    "        # self.load_path8= './data/trained_cppbert8.pt'\n",
    "        \n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert1'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert2'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert3'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert4'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert5'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert6'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert7'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert8'\n",
    "        \n",
    "        self.checkpoint_path = 'neulab/codebert-cpp'\n",
    "        self.test_path = './data/processed_test.pkl'\n",
    "        \n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 설정 클래스를 사용함으로써, 학습 및 테스트 파이프라인을 구축할 때 필요한 모든 설정 값을 한 곳에서 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델, 토크나이저 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.checkpoint_path, num_labels=cfg.num_labels, output_hidden_states=False,ignore_mismatched_sizes=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.checkpoint_path)\n",
    "tokenizer.truncation_side = \"left\"\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> transformers 라이브러리를 사용하여 사전 훈련된 시퀀스 분류 모델을 로드하고, 해당 모델을 특정 작업에 맞게 조정하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(cfg.load_path1))\n",
    "# model.load_state_dict(torch.load(cfg.load_path2))\n",
    "# model.load_state_dict(torch.load(cfg.load_path3))\n",
    "# model.load_state_dict(torch.load(cfg.load_path4))\n",
    "# model.load_state_dict(torch.load(cfg.load_path5))\n",
    "# model.load_state_dict(torch.load(cfg.load_path6))\n",
    "# model.load_state_dict(torch.load(cfg.load_path7))\n",
    "# model.load_state_dict(torch.load(cfg.load_path8))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 모델을 평가 모드로 전환하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"local이 아닌 허깅페이스에 저장된 모델 불러오려면 다음과 같이 해야 합니다\"\"\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(cfg.hf_load_path1, num_labels=cfg.num_labels, output_hidden_states=False).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(cfg.checkpoint_path,)\n",
    "\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "'''DataParallel이 필요할 경우에는 아래 코드를 실행해야 합니다'''\n",
    "# model = nn.DataParallel(model).to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(cfg.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code1과 code2 열의 값을 numpy 배열로 추출\n",
    "c1 = test_data['code1'].values\n",
    "c2 = test_data['code2'].values\n",
    "\n",
    "#N은 테스트 데이터셋의 샘플 수를 나타내며, MAX_LEN은 모델 입력에 사용될 시퀀스의 최대 길이를 설정\n",
    "N = test_data.shape[0]\n",
    "MAX_LEN = 512\n",
    "\n",
    "#test_input_ids와 test_attention_masks를 각각 N x MAX_LEN 크기의 정수형(numpy int) 0 행렬로 초기화\n",
    "test_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "test_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "\n",
    "#진행 상황을 보여주는 tqdm 라이브러리를 사용하여 N번 반복\n",
    "for i in tqdm(range(N), position=0, leave=True):\n",
    "    try:\n",
    "        cur_c1 = str(c1[i])\n",
    "        cur_c2 = str(c2[i])\n",
    "\n",
    "        #토크나이저를 사용하여 cur_c1과 cur_c2 코드 조각을 토큰화하고, PyTorch 텐서로 변환된 입력을 생성\n",
    "        encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                    truncation=True)\n",
    "        \n",
    "        #토큰화된 입력에서 input_ids와 attention_mask를 추출하여 각각 test_input_ids와 test_attention_masks의 i번째 행에 할\n",
    "        test_input_ids[i,] = encoded_input['input_ids']\n",
    "        test_attention_masks[i,] = encoded_input['attention_mask']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids, dtype=int)\n",
    "test_attention_masks = torch.tensor(test_attention_masks, dtype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 두 코드 조각(code1과 code2)을 포함하는 테스트 데이터셋에 대해 입력 ID와 주의(attention) 마스크를 생성하는 과정을 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''토큰화 작업에 시간이 오래걸려 토큰 파일만 따로 저장해두고 불러와서 추론했습니다'''\n",
    "torch.save(test_input_ids, \"./data/test_input_ids.pt\")\n",
    "torch.save(test_attention_masks, \"./data/test_attention_masks.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids=torch.load('./data/test_input_ids.pt')\n",
    "test_attention_masks=torch.load('./data/test_attention_masks.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda()\n",
    "\n",
    "#TensorDataset 클래스를 사용하여 test_input_ids와 test_attention_masks로부터 텐서 데이터셋을 생성\n",
    "test_tensor = TensorDataset(test_input_ids, test_attention_masks)\n",
    "\n",
    "#SequentialSampler를 사용하여 테스트 데이터셋의 샘플링 방법을 순차적으로 설정\n",
    "test_sampler = SequentialSampler(test_tensor)\n",
    "\n",
    "#DataLoader를 사용하여 배치 크기가 16인 테스트 데이터 로더를 생성\n",
    "test_dataloader = DataLoader(test_tensor, sampler=test_sampler, batch_size=16)\n",
    "\n",
    "submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "# 모델 출력(로짓)을 저장할 리스트를 초기\n",
    "logits_list = [] #soft voting을 위한 리스트\n",
    "preds = np.array([]) #최종 출력값(레이블)\n",
    "\n",
    "#반복문을 통해 test_dataloader에서 배치를 순차적으로 가져와 모델로부터 예측을 수행\n",
    "for step, batch in tqdm(enumerate(test_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "\n",
    "    #각 배치를 설정된 디바이스(GPU 또는 CPU)로 이동\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    #배치에서 입력 ID와 주의 마스크를 추출\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    #이 구문 내에서 모델의 예측을 수행할 때, PyTorch가 자동으로 계산 그래프를 생성하는 것을 방지하여 메모리 사용량을 감소시키고 계산 속도를 향상\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #모델에 입력 ID와 주의 마스크를 제공하여 예측을 수행\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "    \n",
    "    '''soft voting을 위한 logit값'''\n",
    "\n",
    "    #모델 출력 중 첫 번째 요소를 로짓으로 추출\n",
    "    logits = outputs[0]\n",
    "\n",
    "    #로짓을 계산 그래프로부터 분리하고, CPU로 이동\n",
    "    logits = logits.detach().cpu()\n",
    "\n",
    "    #분리된 로짓을 logits_list에 추가\n",
    "    logits_list.append(logits)\n",
    "    \n",
    "    '''최종 출력label(0 & 1)'''\n",
    "    _pred = logits.numpy()\n",
    "    pred = np.argmax(_pred, axis=1).flatten()\n",
    "    preds = np.append(preds, pred)\n",
    "    \n",
    "submission['similar'] = preds\n",
    "all_logits = torch.cat(logits_list, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 테스트 데이터셋에 대한 모델의 예측을 수행하고, 그 결과를 제출 파일 형식으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_logits, \"./data/all_logits_model1.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model2.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model3.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model4.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model5.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model6.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model7.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model8.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./data/submission' +'model1.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model2.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model3.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model4.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model5.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model6.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model7.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model8.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble(Soft Voting)\n",
    "\n",
    "- Soft Voting 전 Logit 값들을 모두 Softmax 통과 시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "#logits_1부터 logits_8까지 각 줄은 모델 1부터 모델 8까지 각각의 로짓 파일을 불러온 후, PyTorch의 softmax 함수를 사용하여 로짓을 확률로 변환\n",
    "logits_1 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model1.pt\"))\n",
    "logits_2 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model2.pt\"))\n",
    "logits_3 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model3.pt\"))\n",
    "logits_4 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model4.pt\"))\n",
    "logits_5 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model5.pt\"))\n",
    "logits_6 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model6.pt\"))\n",
    "logits_7 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model7.pt\"))\n",
    "logits_8 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model8.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ->예측 확률을 불러오고, 이를 확률로 변환하여 나중에 이러한 확률을 기반으로 최종 예측을 결정하는 데 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#여덟 개 모델의 소프트맥스 확률(logits_1부터 logits_8까지)을 모두 더한 후, 8로 나누어 평균을 계산\n",
    "logits = (logits_1 + logits_2 + logits_3 + logits_4 + logits_5 + logits_6 + logits_7 + logits_8) / 8\n",
    "\n",
    "# logits를 NumPy 배열로 변환\n",
    "logits_np = logits.numpy()\n",
    "\n",
    "#np.argmax 함수를 사용하여 변환된 NumPy 배열 logits_np의 각 행(각 예측 확률 벡터)에 대해 가장 높은 값을 가진 인덱스를 찾습니다.\n",
    "pred = np.argmax(logits_np, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> 여러 모델의 예측을 통합하여 최종 예측을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['similar'] = pred\n",
    "submission.to_csv('./data/final_soft_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY39_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
