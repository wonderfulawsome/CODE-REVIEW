{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas \n",
    "!pip install scikit-learn \n",
    "!pip install numba \n",
    "!pip install tqdm \n",
    "!pip install matplotlib \n",
    "!pip install transformers\n",
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import random\n",
    "import os, re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import logging\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoModelForSequenceClassification\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from itertools import combinations\n",
    "from rank_bm25 import BM25L\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 생성\n",
    "- 월간 데이콘 코드 유사성 판단 AI 경진대회 시즌 1 - 청소님 코드 참고하였습니다!\n",
    "- 전처리 코드 CPP에 맞게 수정하였습니다\n",
    "- 약 1억 건의 데이터가 생성됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 자동으로 불러오기 위한 숫자 생성(001~500)\n",
    "numbers = [str(i).zfill(3) for i in range(1, 501)]\n",
    "all_code_list = []\n",
    "\n",
    "# train_code(cpp) 불러오기\n",
    "for i in numbers:\n",
    "    cpp_code_list = []\n",
    "    \n",
    "    for file in os.listdir(f\"./data/train_code/problem{i}\"):\n",
    "    \n",
    "        if file.endswith(\".cpp\"):\n",
    "            with open(os.path.join(f\"./data/train_code/problem{i}\", file), \"r\") as f:\n",
    "                cpp_code_list.append(f.read())\n",
    "    all_code_list.append(cpp_code_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 1_주석 처리 \n",
    "def clean_data(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"//.*\", \"\", text)\n",
    "    text = re.sub(r'/\\*.*?\\*/', '', text, flags=re.DOTALL)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# 전처리 함수 2_빈 줄 제거\n",
    "def get_rid_of_empty(c):\n",
    "    ret = []\n",
    "    splitted = c.split('\\n')\n",
    "    for s in splitted:\n",
    "        if len(s.strip()) > 0:\n",
    "            ret.append(s)\n",
    "    return '\\n'.join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 실행\n",
    "all_code_list_clean = []\n",
    "\n",
    "for i in range(500):\n",
    "    cleans = []\n",
    "    for j in range(500):\n",
    "        clean = get_rid_of_empty(clean_data(all_code_list[i][j]))\n",
    "        cleans.append(clean)\n",
    "    all_code_list_clean.append(cleans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 생성 config 및 함수\n",
    "\n",
    "# config\n",
    "class cfg():\n",
    "    def __init__(self) :\n",
    "        self.checkpoint_path = 'neulab/codebert-cpp'\n",
    "        # self.learning_rate = 3e-4\n",
    "        # self.epochs = 5\n",
    "        # self.num_labels=2\n",
    "        # self.batch_size=16\n",
    "\n",
    "args = cfg()\n",
    "\n",
    "# 함수\n",
    "def get_pairs(input_df, tokenizer):\n",
    "    codes = input_df['code'].to_list()\n",
    "    problems = input_df['problem_num'].unique().tolist()\n",
    "    problems.sort()\n",
    "\n",
    "    tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "    bm25 = BM25L(tokenized_corpus)\n",
    "\n",
    "    total_positive_pairs = []\n",
    "    total_negative_pairs = []\n",
    "\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes = input_df[input_df['problem_num'] == problem]['code']\n",
    "        positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "        solution_codes_indices = solution_codes.index.to_list()\n",
    "        negative_pairs = []\n",
    "\n",
    "        first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "        negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "        negative_code_ranking = negative_code_scores.argsort()[::-1] # 내림차순\n",
    "        ranking_idx = 0\n",
    "\n",
    "        for solution_code in solution_codes:\n",
    "            negative_solutions = []\n",
    "            while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "                high_score_idx = negative_code_ranking[ranking_idx]\n",
    "\n",
    "                if high_score_idx not in solution_codes_indices:\n",
    "                    negative_solutions.append(input_df['code'].iloc[high_score_idx])\n",
    "                ranking_idx += 1\n",
    "\n",
    "            for negative_solution in negative_solutions:\n",
    "                negative_pairs.append((solution_code, negative_solution))\n",
    "\n",
    "        total_positive_pairs.extend(positive_pairs)\n",
    "        total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "    pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "    pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "    neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "    neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "    pos_label = [1]*len(pos_code1)\n",
    "    neg_label = [0]*len(neg_code1)\n",
    "\n",
    "    pos_code1.extend(neg_code1)\n",
    "    total_code1 = pos_code1\n",
    "    pos_code2.extend(neg_code2)\n",
    "    total_code2 = pos_code2\n",
    "    pos_label.extend(neg_label)\n",
    "    total_label = pos_label\n",
    "    pair_data = pd.DataFrame(data={\n",
    "        'code1':total_code1,\n",
    "        'code2':total_code2,\n",
    "        'similar':total_label\n",
    "    })\n",
    "    pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "    return pair_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 셋 만들기 (각 코드 : 문제 번호)\n",
    "preproc_scripts = []\n",
    "problem_nums = []\n",
    "\n",
    "for i in range(500):\n",
    "    for k in range(500):\n",
    "        preprocessed_script = all_code_list_clean[i][k]\n",
    "        preproc_scripts.append(preprocessed_script)\n",
    "        problem_nums.append(numbers[i])\n",
    "        \n",
    "data_df = pd.DataFrame(data={'code': preproc_scripts, 'problem_num': problem_nums})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_code, valid_code, train_label, valid_label = train_test_split(\n",
    "    data_df,\n",
    "    data_df['problem_num'],\n",
    "    random_state=42,\n",
    "    test_size=0.1,\n",
    "    stratify=data_df['problem_num']\n",
    ")\n",
    "\n",
    "train_code = train_code.reset_index(drop=True)\n",
    "valid_code = valid_code.reset_index(drop=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "final_train_df = get_pairs(train_code, tokenizer)\n",
    "# final_valid_df = get_pairs(valid_code, tokenizer)\n",
    "\n",
    "# 생성 데이터 저장\n",
    "final_train_df.to_pickle(\"./data/train.pkl\")\n",
    "# final_valid_df.to_pickle(\"./data/val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 셋 만들기\n",
    "test_df = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "code1 = test_df['code1'].values\n",
    "code2 = test_df['code2'].values\n",
    "processed_code1 = []\n",
    "processed_code2 = []\n",
    "for i in range(len(code1)):\n",
    "        processed_c1 = get_rid_of_empty(clean_data(code1[i]))\n",
    "        processed_c2 = get_rid_of_empty(clean_data(code2[i]))\n",
    "        processed_code1.append(processed_c1)\n",
    "        processed_code2.append(processed_c2)\n",
    "\n",
    "processed_test = pd.DataFrame(list(zip(processed_code1, processed_code2)), columns=[\"code1\", \"code2\"])\n",
    "processed_test.to_pickle(\"./data/processed_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "train_data = pd.read_pickle(\"./data/train.pkl\")\n",
    "# val_data = pd.read_pickle(\"./data/val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 추출(랜덤)\n",
    "def data_splitter(df, size):\n",
    "    label_0_df = df[df['similar'] == 0].sample(size // 2)\n",
    "    label_0_idx = label_0_df.index\n",
    "\n",
    "    label_1_df = df[df['similar'] == 1].sample(size // 2)\n",
    "    label_1_idx = label_1_df.index\n",
    "\n",
    "    sampled_df = pd.concat([label_0_df, label_1_df], axis=0)\n",
    "    sampled_idx = list(label_0_idx) + list(label_1_idx)\n",
    "\n",
    "    return sampled_idx, sampled_df\n",
    "\n",
    "# 추출한 후 데이터 셋을 초기화\n",
    "def splitted_original(origin, idx):\n",
    "    origin = origin.drop(idx)\n",
    "    return origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 5000000\n",
    "\n",
    "for i in range(1, 9):\n",
    "    train_idx, train_df = data_splitter(train_data, train_size)\n",
    "    train_data = splitted_original(train_data, train_idx)\n",
    "    train_df.to_pickle(f'./data/train{i}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_size = 100000\n",
    "\n",
    "# valid_idx, valid_df = data_splitter(val_data, val_size)\n",
    "# val_data = splitted_original(val_data, valid_idx)\n",
    "# valid_df.to_pickle(f'./data/valid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "- 모델은 neulab/codebert-cpp입니다.\n",
    "- 다른 데이터 8개로 모델 파인 튜닝시킵니다.\n",
    "- DataParallel 시 loss 부분이랑 모델 저장 부분에서 주의해야합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.source_len=512\n",
    "        self.epochs = 1\n",
    "        self.learning_rate=2e-5\n",
    "        self.batch_size=32\n",
    "        self.shuffle = True\n",
    "        self.seed=2022\n",
    "        self.num_labels=2\n",
    "        self.checkpoint_path = 'neulab/codebert-cpp'\n",
    "        self.train_path1 = './data/train1.pkl'\n",
    "        # self.train_path2 = './data/train2.pkl'\n",
    "        # self.train_path3 = './data/train3.pkl'\n",
    "        # self.train_path4 = './data/train4.pkl'\n",
    "        # self.train_path5 = './data/train5.pkl'\n",
    "        # self.train_path6 = './data/train6.pkl'\n",
    "        # self.train_path7 = './data/train7.pkl'\n",
    "        # self.train_path8 = './data/train8.pkl'\n",
    "        \n",
    "        # self.hf_data_path1= 'emaeon/train1'\n",
    "        # self.hf_data_path2= 'emaeon/train2'\n",
    "        # self.hf_data_path3= 'emaeon/train3'\n",
    "        # self.hf_data_path4= 'emaeon/train4'\n",
    "        # self.hf_data_path5= 'emaeon/train5'\n",
    "        # self.hf_data_path6= 'emaeon/train6'\n",
    "        # self.hf_data_path7= 'emaeon/train7'\n",
    "        # self.hf_data_path8= 'emaeon/train8'\n",
    "\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.bechmark = True\n",
    "\n",
    "seed_everything(cfg.seed) #seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델, 토크나이저 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스에서 사전학습된 모델 불러옵니다\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.checkpoint_path, num_labels=cfg.num_labels, output_hidden_states=False).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.checkpoint_path,)\n",
    "\n",
    "tokenizer.truncation_side = \"left\"\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "'''DataParallel이 필요할 경우에는 아래 코드를 실행해야 합니다'''\n",
    "# model = nn.DataParallel(model).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model #모델 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 커스터마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_a,data_b, labels, tokenizer, source_len) :\n",
    "    # 내가 필요한 것들을 가져와서 선처리\n",
    "        self.data_a = data_a.copy()\n",
    "        self.data_b = data_b.copy()\n",
    "        self.labels = labels.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_len = source_len\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "    # 데이터 셋에서 한 개의 데이터를 가져오는 함수 정의\n",
    "\n",
    "        text1 = self.data_a[index]\n",
    "        text2 = self.data_b[index]\n",
    "        \n",
    "        '''text_pair에 비교할 문장을 입력하면 알아서 sep토큰으로 문장 구분된 하나의 입력 셋이 형성됩니다.'''\n",
    "        inputs = self.tokenizer(text = text1,text_pair=text2,max_length=self.source_len,padding='max_length',truncation=True, return_tensors='pt') \n",
    "        label = self.labels[index]\n",
    "\n",
    "        '''neulab/codebert-cpp는 input_ids와 attention_mask를 입력 받습니다'''\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        inputs_dict = {\n",
    "            'input_ids' : input_ids.to(device, dtype = torch.long),\n",
    "            'attention_mask' : attention_mask.to(device, dtype = torch.long),\n",
    "        }\n",
    "        label = torch.tensor(label).to(device, dtype = torch.long)\n",
    "\n",
    "\n",
    "        return inputs_dict, label #\n",
    "\n",
    "    def __len__(self) :\n",
    "    # 데이터 셋의 길이\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_pickle(cfg.train_path1)\n",
    "# traindf = pd.read_pickle(cfg.train_path2)\n",
    "# traindf = pd.read_pickle(cfg.train_path3)\n",
    "# traindf = pd.read_pickle(cfg.train_path4)\n",
    "# traindf = pd.read_pickle(cfg.train_path5)\n",
    "# traindf = pd.read_pickle(cfg.train_path6)\n",
    "# traindf = pd.read_pickle(cfg.train_path7)\n",
    "# traindf = pd.read_pickle(cfg.train_path8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"허깅페이스에 올려놓은 데이터를 가져오려면 다음과 같이 해야 합니다\"\"\"\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(cfg.hf_data_path1)\n",
    "# traindf = dataset['train'].to_pandas()\n",
    "\n",
    "\n",
    "# dataset = load_dataset(cfg.hf_data_path2)\n",
    "# dataset = load_dataset(cfg.hf_data_path3)\n",
    "# dataset = load_dataset(cfg.hf_data_path4)\n",
    "# dataset = load_dataset(cfg.hf_data_path5)\n",
    "# dataset = load_dataset(cfg.hf_data_path6)\n",
    "# dataset = load_dataset(cfg.hf_data_path7)\n",
    "# dataset = load_dataset(cfg.hf_data_path8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(data_a=list(traindf['code1']),data_b=list(traindf['code2']),\n",
    "                           labels=list(traindf['similar']),tokenizer=tokenizer,source_len=cfg.source_len)\n",
    "train_loader = DataLoader(train_data, batch_size=cfg.batch_size, shuffle=cfg.shuffle,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, loader):\n",
    "\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    nb_train_steps = 0\n",
    "    \n",
    "    for _,(inputs, labels) in tqdm(enumerate(loader, 0)):\n",
    "        \n",
    "        outputs = model(**inputs, labels = labels)\n",
    "        loss = outputs.loss\n",
    "        '''DataParallel은 loss가 3개가 나오기 때문에 평균해야 합니다'''\n",
    "        # loss = outputs.loss.mean()\n",
    "\n",
    "        pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "        true = [label for label in labels.cpu().numpy()]\n",
    "        acc = accuracy_score(true,pred)\n",
    "\n",
    "        if _ % 50 == 0 and not _ == 0: #50iter마다 loss 확인하고자 넣었습니다.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(_, len(loader), elapsed))\n",
    "            print('  current average loss = {}'.format(\n",
    "                total_loss / _))\n",
    "        if _%50==0  and not _ == 0 :\n",
    "            print(f'Epoch : {epoch}, train_{_}_step_loss : {loss.item()}')\n",
    "            psuedo_pred = [logit.argmax().item() for logit in outputs.logits]\n",
    "            psuedo_acc = np.sum(np.array(labels.to('cpu'))==np.array(psuedo_pred))/len(labels)\n",
    "            print(f'{epoch}_{_}_step_정확도 :{psuedo_acc}')\n",
    "        if _%15625==0 and not _ == 0: #런타임 오류가 생길 경우 모델이 날라갈 것을 방지하고자 했습니다.\n",
    "            torch.save(model.state_dict(), f'/data/{_}batch_trained_cppbert1.pt')\n",
    "            '''DataParallel후 저장하는 방법이 약간 다릅니다'''\n",
    "            # torch.save(model.module.state_dict(), f'/data/{_}batch_trained_cppbert.pt')\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += acc\n",
    "        nb_train_steps += 1\n",
    "\n",
    "    avg_loss = total_loss/len(loader)\n",
    "    avg_acc = total_accuracy/nb_train_steps\n",
    "    t_test_avg_acc = total_accuracy/len(loader)\n",
    "    print(f'Epoch:{epoch}, train_{_}_stepLoss:{avg_loss}')\n",
    "    print(f'Epoch:{epoch}, train_{_}_stepacc:{avg_acc}')\n",
    "    print(f'Epoch:{epoch}, train_{_}_stepacc:{t_test_avg_acc}')\n",
    "    loss_dic['train_loss'].append(avg_loss)\n",
    "    loss_dic['train_acc'].append(avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr=cfg.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.01, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dic = {'epoch':[],'train_loss':[], 'validation_loss':[],'train_acc':[],'val_acc':[]}\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(1,cfg.epochs+1)):\n",
    "    t0 = time.time()\n",
    "    train(epoch, model, optimizer, train_loader)\n",
    "    torch.save(model.state_dict(), './data/trained_cppbert1.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert2.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert3.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert4.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert5.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert6.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert7.pt')\n",
    "    # torch.save(model.state_dict(), './data/trained_cppbert8.pt')\n",
    "    \n",
    "    '''DataParallel후 저장하는 방법이 약간 다릅니다'''\n",
    "    # torch.save(model.module.state_dict(), './data/trained_cppbert1.pt')  \n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "- 월간 데이콘 코드 유사성 판단 AI 경진대회 시즌 1 - Gmin47님 코드 참고하였습니다!\n",
    "- Soft Voting을 위해 Logit값을 저장해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.source_len=512\n",
    "        self.batch_size=16\n",
    "        self.shuffle = True\n",
    "        self.seed=2022\n",
    "        self.num_labels=2\n",
    "        self.load_path1= './data/trained_cppbert1.pt'\n",
    "        # self.load_path2= './data/trained_cppbert2.pt'\n",
    "        # self.load_path3= './data/trained_cppbert3.pt'\n",
    "        # self.load_path4= './data/trained_cppbert4.pt'\n",
    "        # self.load_path5= './data/trained_cppbert5.pt'\n",
    "        # self.load_path6= './data/trained_cppbert6.pt'\n",
    "        # self.load_path7= './data/trained_cppbert7.pt'\n",
    "        # self.load_path8= './data/trained_cppbert8.pt'\n",
    "        \n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert1'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert2'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert3'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert4'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert5'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert6'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert7'\n",
    "        # self.hf_load_path1= 'emaeon/trained_cppbert8'\n",
    "        \n",
    "        self.checkpoint_path = 'neulab/codebert-cpp'\n",
    "        self.test_path = './data/processed_test.pkl'\n",
    "        \n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델, 토크나이저 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.checkpoint_path, num_labels=cfg.num_labels, output_hidden_states=False,ignore_mismatched_sizes=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.checkpoint_path)\n",
    "tokenizer.truncation_side = \"left\"\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(cfg.load_path1))\n",
    "# model.load_state_dict(torch.load(cfg.load_path2))\n",
    "# model.load_state_dict(torch.load(cfg.load_path3))\n",
    "# model.load_state_dict(torch.load(cfg.load_path4))\n",
    "# model.load_state_dict(torch.load(cfg.load_path5))\n",
    "# model.load_state_dict(torch.load(cfg.load_path6))\n",
    "# model.load_state_dict(torch.load(cfg.load_path7))\n",
    "# model.load_state_dict(torch.load(cfg.load_path8))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"local이 아닌 허깅페이스에 저장된 모델 불러오려면 다음과 같이 해야 합니다\"\"\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(cfg.hf_load_path1, num_labels=cfg.num_labels, output_hidden_states=False).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(cfg.checkpoint_path,)\n",
    "\n",
    "# tokenizer.truncation_side = \"left\"\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "'''DataParallel이 필요할 경우에는 아래 코드를 실행해야 합니다'''\n",
    "# model = nn.DataParallel(model).to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(cfg.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = test_data['code1'].values\n",
    "c2 = test_data['code2'].values\n",
    "\n",
    "N = test_data.shape[0]\n",
    "MAX_LEN = 512\n",
    "\n",
    "test_input_ids = np.zeros((N, MAX_LEN), dtype=int)\n",
    "test_attention_masks = np.zeros((N, MAX_LEN), dtype=int)\n",
    "\n",
    "for i in tqdm(range(N), position=0, leave=True):\n",
    "    try:\n",
    "        cur_c1 = str(c1[i])\n",
    "        cur_c2 = str(c2[i])\n",
    "        encoded_input = tokenizer(cur_c1, cur_c2, return_tensors='pt', max_length=512, padding='max_length',\n",
    "                                    truncation=True)\n",
    "        test_input_ids[i,] = encoded_input['input_ids']\n",
    "        test_attention_masks[i,] = encoded_input['attention_mask']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "test_input_ids = torch.tensor(test_input_ids, dtype=int)\n",
    "test_attention_masks = torch.tensor(test_attention_masks, dtype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''토큰화 작업에 시간이 오래걸려 토큰 파일만 따로 저장해두고 불러와서 추론했습니다'''\n",
    "torch.save(test_input_ids, \"./data/test_input_ids.pt\")\n",
    "torch.save(test_attention_masks, \"./data/test_attention_masks.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids=torch.load('./data/test_input_ids.pt')\n",
    "test_attention_masks=torch.load('./data/test_attention_masks.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda()\n",
    "\n",
    "test_tensor = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_sampler = SequentialSampler(test_tensor)\n",
    "test_dataloader = DataLoader(test_tensor, sampler=test_sampler, batch_size=16)\n",
    "\n",
    "submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "logits_list = [] #soft voting을 위한 리스트입니다\n",
    "preds = np.array([]) #최종 출력값(레이블)입니다\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(test_dataloader), desc=\"Iteration\", smoothing=0.05):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "    \n",
    "    '''soft voting을 위한 logit값'''\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu()\n",
    "    logits_list.append(logits)\n",
    "    \n",
    "    '''최종 출력label(0 & 1)'''\n",
    "    _pred = logits.numpy()\n",
    "    pred = np.argmax(_pred, axis=1).flatten()\n",
    "    preds = np.append(preds, pred)\n",
    "    \n",
    "submission['similar'] = preds\n",
    "all_logits = torch.cat(logits_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_logits, \"./data/all_logits_model1.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model2.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model3.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model4.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model5.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model6.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model7.pt\")\n",
    "# torch.save(all_logits, \"./data/all_logits_model8.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./data/submission' +'model1.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model2.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model3.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model4.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model5.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model6.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model7.csv', index=False)\n",
    "# submission.to_csv('./data/submission' +'model8.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble(Soft Voting)\n",
    "\n",
    "- Soft Voting 전 Logit 값들을 모두 Softmax 통과 시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "logits_1 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model1.pt\"))\n",
    "logits_2 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model2.pt\"))\n",
    "logits_3 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model3.pt\"))\n",
    "logits_4 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model4.pt\"))\n",
    "logits_5 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model5.pt\"))\n",
    "logits_6 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model6.pt\"))\n",
    "logits_7 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model7.pt\"))\n",
    "logits_8 = torch.nn.functional.softmax(torch.load(\"./data/all_logits_model8.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = (logits_1 + logits_2 + logits_3 + logits_4 + logits_5 + logits_6 + logits_7 + logits_8) / 8\n",
    "logits_np = logits.numpy()\n",
    "pred = np.argmax(logits_np, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['similar'] = pred\n",
    "submission.to_csv('./data/final_soft_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY39_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
